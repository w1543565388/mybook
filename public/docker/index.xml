<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Docker on My New Hugo Site</title>
    <link>http://ip:1313/docker/index.html</link>
    <description>Recent content in Docker on My New Hugo Site</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh_CN</language>
    <lastBuildDate>Fri, 12 Apr 2024 09:39:39 +0000</lastBuildDate>
    <atom:link href="http://ip:1313/docker/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Buildah</title>
      <link>http://ip:1313/docker/buildah-hl/index.html</link>
      <pubDate>Fri, 12 Apr 2024 09:39:39 +0000</pubDate>
      <guid>http://ip:1313/docker/buildah-hl/index.html</guid>
      <description>一、编译部署 1、拉取代码 git clone https://gitlabwh.uniontech.com/wuhan/container/buildah2、安装依赖 apt install -y pkg-config libgpgme-dev libseccomp-dev libdevmapper-devgo get github.com/sbinet/go-python3、构建 cd buildah git checkout v1.18.0 make4、复制配置文件 mkdir /etc/containers/ cp docs/samples/registries.conf /etc/containers/ cp tests/policy.json /etc/containers/ cp ./vendor/github.com/containers/storage/storage.conf /etc/containers/5、部署网络插件 git clone https://ghproxy.com/https://github.com/containernetworking/plugins cd ./plugins; ./build_linux.sh mkdir -p /opt/cni/bin install -v ./bin/* /opt/cni/bin6、安装buildah make install或&#xA;cp bin/buildah /usr/local/bin/buildah chmod 755 /usr/local/bin/buildah二、使用 1、根据Dockerfile构建镜像 buildah bud -t test2、推送镜像 buildah push --tls-verify=false --creds admin:123 192.168.0.127:3000/test/test:latest3、挂载已有镜像并修改、重新生成镜像 buildah from localhost/test buildah mount $mycontainer buildah commit $mycontainer containers-storage:myecho24、查看容器 1）在构建容器镜像是的零时容器</description>
    </item>
    <item>
      <title>chartmuseum 相关</title>
      <link>http://ip:1313/docker/chartmuseum-hl/index.html</link>
      <pubDate>Fri, 12 Apr 2024 09:39:39 +0000</pubDate>
      <guid>http://ip:1313/docker/chartmuseum-hl/index.html</guid>
      <description>chartmuseum github&#xA;一、安装 chartmuseum 1、添加 helm 源 helm repo add stable https://charts.helm.sh/stable2、拉取 chart 包 helm pull stable/chartmuseum --untar3、修改配置 vim chartmuseum/value.yaml DISABLE_API: false type: NodePort4、部署 helm install chartmuseum/二、通过 helm 添加使用 helm repo add chartmuseum http://localhost:8080helm search repo chartmuseumhelm install chartmuseum/mycharthelm push mychart/ chartmuseum三、通过 curl 命令，进行 CRUD 操作 1、查看仓库信息 curl http://localhost:8080/index.yaml2、查看所有软件 curl http://localhost:8080/api/charts3、查看某个软件的所有版本信息 curl http://localhost:8080/api/charts/nginx4、查看某个软件的具体版本的信息 curl http://localhost:8080/api/charts/nginx/5.1.55、下载软件包 curl -O http://localhost:8080/charts/nginx-5.1.5.tgz6、上传软件包到仓库 curl --data-binary &amp;#34;@rancher-2.5.1.tgz&amp;#34; http://localhost:8080/api/charts curl -X POST -k --data-binary &amp;#34;@mychart-1.0-0.1.1.tgz&amp;#34; localhost:8080/api/charts7、删除仓库中的软件 curl -X DELETE http://localhost:8080/api/charts/nginx/5.</description>
    </item>
    <item>
      <title>Containerd</title>
      <link>http://ip:1313/docker/containerd-hl/index.html</link>
      <pubDate>Fri, 12 Apr 2024 09:39:39 +0000</pubDate>
      <guid>http://ip:1313/docker/containerd-hl/index.html</guid>
      <description>一、Containerd TODO&#xA;二、ctr 1、拉取 ctr i pull docker.io/library/ubuntu:latest ctr -n k8s.io i pull --plain-http=true 192.168.0.31:30002/library/ubuntu_armv8_edge:1.1 https_proxy=http://192.168.0.169:1080 http_proxy=http://192.168.0.169:1080 ctr -n k8s.io i pull k8s.gcr.io/pause:3.22、查看 ctr c ls ctr -n k8s.io i ls -q3、运行 ctr run -t docker.io/library/ubuntu:latest test bash ctr run -d docker.io/library/ubuntu:latest test bash4、删除 ctr c rm test5、导入导出镜像 bzip2 -cd aaa_v1.0.0_image_amd64.tar.bz2 | k3s ctr -n k8s.io i import -docker save aaa:v1.0.0 | bzip2 &amp;gt; aaa_v1.0.0_image_amd64.tar.bz2三、crictl TODO&#xA;四、其他 1、解压OCI镜像 ctr i export - docker.</description>
    </item>
    <item>
      <title>docker compose 相关</title>
      <link>http://ip:1313/docker/docker_compose-hl/index.html</link>
      <pubDate>Fri, 12 Apr 2024 09:39:39 +0000</pubDate>
      <guid>http://ip:1313/docker/docker_compose-hl/index.html</guid>
      <description>一、介绍 略&#xA;二、docker-compose &amp;ndash;help build Build or rebuild services bundle Generate a Docker bundle from the Compose file config Validate and view the Compose file create Create services down Stop and remove containers, networks, images, and volumes events Receive real time events from containers exec Execute a command in a running container help Get help on a command images List images kill Kill containers logs View output from containers pause Pause services port Print the public port for a port binding ps List containers pull Pull service images push Push service images restart Restart services rm Remove stopped containers run Run a one-off command scale Set number of containers for a service start Start services stop Stop services top Display the running processes unpause Unpause services up Create and start containers version Show the Docker-Compose version information三 、常用命令 docker-compose up -d nginx 构建建启动nignx容器 docker-compose exec nginx bash 登录到nginx容器中 docker-compose down 删除所有nginx容器,镜像 docker-compose ps 显示所有容器 docker-compose restart nginx 重新启动nginx容器 docker-compose build nginx 构建镜像 docker-compose build --no-cache nginx 不带缓存的构建 docker-compose logs nginx 查看nginx的日志 docker-compose logs -f nginx 查看nginx的实时日志 docker-compose config -q 验证（docker-compose.</description>
    </item>
    <item>
      <title>docker registry 相关</title>
      <link>http://ip:1313/docker/docker_registry-hl/index.html</link>
      <pubDate>Fri, 12 Apr 2024 09:39:39 +0000</pubDate>
      <guid>http://ip:1313/docker/docker_registry-hl/index.html</guid>
      <description>一、docker registry 1、拉取registry镜像 docker pull registry2、启动registry镜像 docker run -d --name registry -p 4000:5000 --restart=always -v /opt/registry/:/var/lib/registry/docker/registry registry:latest3、测试，拉取busybox镜像，push到registy中 docker pull busybox docker tag busybox 127.0.0.1:4000/busybox docker push 127.0.0.1:4000/busybox4、查看registry仓库 registry api&#xA;curl http://127.0.0.1:4000/v2/_catalog curl http://127.0.0.1:4000/v2/contrail-agent-u14.04/tags/list curl http://127.0.0.1:4000/v2/kolla/centos-source-nova-api/tags/list curl http://127.0.0.1:4000/v2/flannel/manifests/latest5、配置docker registy tee /etc/docker/daemon.json &amp;lt;&amp;lt;-&amp;#39;EOF&amp;#39; { &amp;#34;insecure-registries&amp;#34;: [&amp;#34;127.0.0.1:4000&amp;#34;] } EOF systemctl daemon-reload systemctl restart docker6、打包registry镜像文件 tar -zcvf registry-20190618.tar.gz /opt/registry/7、再次使用 tar -zxvf registry-20190618.tar.gz -C /opt/ docker run -d --name registry -p 4000:5000 --restart=always -v /opt/registry/:/var/lib/registry/docker/registry registry:latest二、docker harbor doker harbor</description>
    </item>
    <item>
      <title>docker swarm 相关</title>
      <link>http://ip:1313/docker/docker_swarm-hl/index.html</link>
      <pubDate>Fri, 12 Apr 2024 09:39:39 +0000</pubDate>
      <guid>http://ip:1313/docker/docker_swarm-hl/index.html</guid>
      <description>一、docker swarm常用命令 docker swarm集群开放了三个端口：&#xA;2377端口， 用于集群管理通信&#xA;7946端口， 用于集群节点之间的通信&#xA;4789端口， 用于overlay网络流量&#xA;1、初始化swarm manager并制定网卡地址 docker swarm init --advertise-addr 192.168.10.10 Swarm initialized: current node (anvkkvtgrxloqwleiyjjsw2gh) is now a manager. To add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-08ejd2bmn0yhns2cvgkpa0vnu095i4yc2t8jlbdqyvd3tekx6h-e5ore8jctvoycgw2vb1elne2z 192.168.110.82:2377 To add a manager to this swarm, run &amp;#39;docker swarm join-token manager&amp;#39; and follow the instructions.2、强制删除集群，如果是manager，需要添加 &amp;ndash;force、 docker swarm leave --force docker node rm node1 --force # 在manager节点上执行 docker swarm update#####3、查看swarm worker的连接令牌</description>
    </item>
    <item>
      <title>docker 相关</title>
      <link>http://ip:1313/docker/docker-hl/index.html</link>
      <pubDate>Fri, 12 Apr 2024 09:39:39 +0000</pubDate>
      <guid>http://ip:1313/docker/docker-hl/index.html</guid>
      <description>一、常用 1、在线安装docker aliyun 安装docker-ce&#xA;tsinghua 安装docker-ce&#xA;curl -sSL https://get.docker.io | bashcurl -fsSL https://get.docker.com | bash -s docker --mirror Aliyundocker --version docker info2、二进制安装 wget https://download.docker.com/linux/static/stable/aarch64/docker-20.10.8.tgz #wget https://download.docker.com/linux/static/stable/x86_64/docker-20.10.8.tgz tar -zxvf docker-20.10.8.tgz cp docker/* /usr/bin/ dockerd &amp;amp; cat /etc/systemd/system/docker.service&#xA;[Unit] Description=Docker Application Container Engine Documentation=https://docs.docker.com After=network-online.target docker.socket Wants=network-online.target Requires=docker.socket [Service] Type=notify # the default is not to use systemd for cgroups because the delegate issues still # exists and systemd currently does not support the cgroup feature set required # for containers run by docker ExecStart=/usr/bin/dockerd -H fd:// ExecReload=/bin/kill -s HUP $MAINPID LimitNOFILE=1048576 # Having non-zero Limit*s causes performance problems due to accounting overhead # in the kernel.</description>
    </item>
    <item>
      <title>docker 网络相关</title>
      <link>http://ip:1313/docker/docker_net-hl/index.html</link>
      <pubDate>Fri, 12 Apr 2024 09:39:39 +0000</pubDate>
      <guid>http://ip:1313/docker/docker_net-hl/index.html</guid>
      <description>一、docker 五种网络模式 在docker run &amp;ndash;net选项指定容器网络模式，docker有以下4种网络模式：&#xA;docker使用linux的namespace技术来进行资源隔离，如pid namespace隔离进程，mount namespace隔离文件系统，network namespace提供一份独立的网络环境，包括网卡、路由、iptable规则等都与其他的network namespace隔离。一个docker容器一般会分配一个独立的network namespace。&#xA;1、host模式，&amp;ndash;net=host 但如果启动容器的时候使用host模式，那么这个容器将不会获得一个独立的network namespace，而是贺宿主机共用一个network namespace。容器将不会虚拟出自己的网卡、配置自己的ip等，而是使用宿主机的ip和端口。容器进程可以跟主机其它 root 进程一样可以打开低范围的端口，可以访问本地网络服务比如 D-bus，还可以让容器做一些影响整个主机系统的事情，比如重启主机。因此使用这个选项的时候要非常小心。如果进一步的使用 --privileged=true，容器会被允许直接配置主机的网络堆栈。&#xA;例如，我们在10.10.101.105/24的机器上用host模式启动一个含有web应用的docker容器，监听tcp 80端口。在容器中执行任何类似ifconfig命令查看网络环境时，看到的都是宿主机上的信息。而外界访问容器中的应用，则直接使用10.10.101.105:80即可，不用任何nat转换，就如直接跑在宿主机中一样。但是，容器的其他方面，如文件系统、进程列表等还是和宿主机隔离的。&#xA;2、container模式，&amp;ndash;net=container:NAME_or_ID 这个模式指定新创建的容器和已经存在的一个容器共享一个network namespace，而不是和宿主机共享。新创建的容器不会创建自己的网卡、配置自己的ip，而是和一个指定的容器共享ip、端口范围等。同样，两个容器除了网络方面，其他的如文件系统、进程列表等还是隔离的。两个容器的进程可以通过lo回环网卡设备通信。&#xA;3、none模式，&amp;ndash;net=none 这个模式和前两个不同。在这种模式下，docker容器拥有自己的network namespace，但是，并不为docker容器进行任何网络配置。也就是说，这个docker容器，只用lo回环网络，没有网卡、ip、路由等信息。需要我们自己为docker容器添加网卡、配置ip等。这个封闭的网络很好的保证了容器的安全性。&#xA;4、bridge模式，&amp;ndash;net=bridge bridge模式是docker默认的网络设置，此模式会为每一个容器分配network namespace、设置ip等，并将一个主机上的docker容器连接到一个虚拟网桥上。&#xA;1.在主机上创建一对虚拟网卡veth pair设备。veth设备总是成对出现的，它们组成了一个数据的通道，数据从一个设备进入，就会从另一个设备出来。因此，veth设备常用来连接两个网络设备。&#xA;2.docker将veth pair设备的一端放在新创建的容器中，并命名为eth0。另一端放在主机中，以veth65f9这样类似的名字命名，并将这个网络设备加入到docker0网桥中，可以通过brctl show命令查看。&#xA;3.从docker0子网中分配一个IP给容器使用，并设置docker0的IP地址为容器的默认网关。&#xA;5、user-defined模式 用户自定义模式，主要可选的有三种网络驱动：bridge、overlay、macvlan。bridge驱动用于创建类似于前面提到的bridge网络。overlay和macvlan驱动主要用于跨主机的网络。&#xA;overlay模式 overlay网络将多个docker守护进程连接在一起，并使集群服务能够相互通信。还可以使用overlay网络来实现swarm集群和独立容器之间的通信，或者不同docker守护进程上的两个独立容器之间的通信。该策略实现了在这些容器之间进行操作系统级别路由的需求。&#xA;macvlan模式 macvlan网络允许为容器分配mac地址，使其显示为网络上的物理设备。docker守护进程通过其mac地址将流量路由到容器。对于希望直连到物理网络的传统应用程序而言，使用macvlan模式一般是最佳的选择，而不应该通过docker宿主机的网络进行路由。&#xA;二、跨主机通讯模式 docker在垮主机通信方面一直比较弱。目前主要有容器网络模型（CNM）和容器网络接口（CNI）。k8s和docker之间通信采纳的是CNI。&#xA;1、CNM模式（container network model） CNM是一个被 Docker 提出的规范。现在已经被Cisco Contiv, Kuryr, Open Virtual Networking (OVN), Project Calico, VMware 和 Weave 这些公司和项目所采纳。&#xA;libnetwork是CNM的原生实现。它为docker daemon和网络驱动程序之间提供了接口。网络控制器负责将驱动和一个网络进行对接。每个驱动程序负责管理它所拥有的网络以及为该网络提供的各种服务。网络驱动可以按提供方式被划分为原生驱动（libnetwork内置的或docker支持的）或者远程驱动（第三方插件）。原生驱动包括none、bridge、overlay以及macvlan。驱动也可以按照使用范围被划分为本地（单主机）和全局（多主机）。&#xA;2、CNI模式（container network interface） CNI是google和coreos主导制定的容器网络标准，可以理解成一个协议。这个标准是在rkt网络提议基础上发展去来的，综合考虑了灵活性、扩展性、ip分配、多网卡等因素。&#xA;CNI本身实现了一些基本插件，比如bridge、ipvlan、macvlan、loopback、vlan等网络接口管理插件，还有dhcp、host-local等ip管理插件，并且主流的container网络解决方案都有对应CNI的支持能力，比如Flannel、Calico、Weave、Contiv、SR-IOV、Amazon ECS CNI Plugins等。</description>
    </item>
    <item>
      <title>harbor 相关</title>
      <link>http://ip:1313/docker/harbor-hl/index.html</link>
      <pubDate>Fri, 12 Apr 2024 09:39:39 +0000</pubDate>
      <guid>http://ip:1313/docker/harbor-hl/index.html</guid>
      <description>1、安装 docker curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun2、安装 docker-compose curl -L &amp;#34;https://github.com/docker/compose/releases/download/1.25.5/docker-compose-$(uname -s)-$(uname -m)&amp;#34; -o /usr/local/bin/docker-composechmod +x /usr/local/bin/docker-compose3、下载 harbor https://github.com/goharbor/harbor&#xA;wget https://github.com/goharbor/harbor/releases/download/v2.0.0-rc2/harbor-offline-installer-v2.0.0-rc2.tgz4、生成证书 1、生成私钥 openssl genrsa -out ca.key 40962、生成证书 openssl req -x509 -new -nodes -sha512 -days 3650 \ -subj &amp;#34;/C=CN/ST=Chengdu/L=Chengdu/O=example/OU=Personal/CN=local.com&amp;#34; \ -key ca.key \ -out ca.crt5、生成 server 端证书 1、生私钥 openssl genrsa -out local.com.key 40962、生成证书请求文件 openssl req -sha512 -new \ -subj &amp;#34;/C=CN/ST=Chengdu/L=Chengdu/O=example/OU=Personal/CN=local.com&amp;#34; \ -key local.com.key \ -out local.com.csr3、生成 x509 v3 cat &amp;gt; v3.</description>
    </item>
    <item>
      <title>helm 相关</title>
      <link>http://ip:1313/docker/helm-hl/index.html</link>
      <pubDate>Fri, 12 Apr 2024 09:39:39 +0000</pubDate>
      <guid>http://ip:1313/docker/helm-hl/index.html</guid>
      <description>一、快速安装 helm release&#xA;wget https://get.helm.sh/helm-v3.8.0-linux-amd64.tar.gz tar -zxvf helm-*-linux-amd64.tar.gz cp helm /usr/local/bin/helm chmod 777 /usr/local/bin/helmhttps://github.com/chartmuseum/helm-push releases&#xA;wget https://github.com/chartmuseum/helm-push/releases/download/v0.10.2/helm-push_0.10.2_linux_amd64.tar.gz HELM_PUSH=${HOME}/.local/share/helm/plugins mkdir -p ${HELM_PUSH} tar -zxvf helm-push-*.tar.gz -C ${HELM_PUSH} cp registries.yaml /etc/cloud/k8s/ systemctl restart k8s二、Helm v3 三、Helm 客户端安装（v3.0.0） 1、使用官方脚本安装 curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash或&#xA;curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 &amp;gt; get_helm.sh chmod 700 get_helm.sh ./get_helm.sh2、添加常用 Chart 源 #应该都不需要墙，stable是官方的，aliyuncs最快 helm repo add stable https://kubernetes-charts.storage.googleapis.com helm repo add aliyuncs https://apphub.aliyuncs.com helm repo add bitnami https://charts.bitnami.com/bitnami3、查看 Chart 源 helm repo list4、查找应用 helm search repo tomcat5、直接从 Chart 在线安装，需要实现创建动态存储卷等。 helm install my-tomcat aliyuncs/tomcatNAME: my-tomcat LAST DEPLOYED: Thu Dec 5 13:56:04 2019 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ** Please be patient while the chart is being deployed ** 1.</description>
    </item>
    <item>
      <title>k3s 安装与使用</title>
      <link>http://ip:1313/docker/k3s_deploy-hl/index.html</link>
      <pubDate>Fri, 12 Apr 2024 09:39:39 +0000</pubDate>
      <guid>http://ip:1313/docker/k3s_deploy-hl/index.html</guid>
      <description>一、安装 1、安装 curl -sfL https://get.k3s.io | sh - cat /var/lib/rancher/k3s/server/node-token curl -sfL https://get.k3s.io | K3S_URL=https://myserver:6443 K3S_TOKEN=XXX sh - 国内安装：&#xA;server:&#xA;curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn sh -&#xA;node:&#xA;curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn K3S_URL=https://myserver:6443 K3S_TOKEN=mynodetoken sh - 2、 k3s 安装配置 helm curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash cp /etc/rancher/k3s/k3s.yaml $HOME/.kube/config 或 export KUBECONFIG=/etc/rancher/k3s/k3s.yaml helm ls helm repo add aliyuncs https://apphub.aliyuncs.com helm repo add bitnami https://charts.bitnami.com/bitnami # vim /root/.config/helm3、修改k3s使用本地仓库 vim /etc/hosts 192.168.0.242 local.com scp -r 192.</description>
    </item>
    <item>
      <title>k3s 源码编译</title>
      <link>http://ip:1313/docker/k3s_make-hl/index.html</link>
      <pubDate>Fri, 12 Apr 2024 09:39:39 +0000</pubDate>
      <guid>http://ip:1313/docker/k3s_make-hl/index.html</guid>
      <description>一、编译 1、安装 golang wget https://golang.google.cn/dl/go1.15.2.linux-amd64.tar.gz tar -C /usr/local -xzf go1.15.2.linux-amd64.tar.gzvim /etc/profile export PATH=$PATH:/usr/local/go/bin2、获取源码（v1.19.2+k3s1） git clone -b v1.19.2+k3s1 --depth=1 https://github.com/rancher/k3s.git 注：&#xA;&amp;ndash;depth=1 用于指定克隆深度，为1即表示只克隆最近一次commit.&#xA;-b ${branch} clone 某个分支&#xA;3、修改代理 cd ./k3s/vim ./Dockerfile.dapper ARG http_proxy=http://192.168.0.110:1080 ARG https_proxy=http://192.168.0.110:1080 ... trivy --download-db-only --severity HIGH ... curl -x 192.168.0.110:1080 ...vim ./scripts/download curl -x 192.168.0.110:10804、下载生成依赖 mkdir -p build/data &amp;amp;&amp;amp; ./scripts/download &amp;amp;&amp;amp; go generate5、编译 SKIP_VALIDATE=true make 1、SKIP_VALIDATE=true，因为修改了源文件，不跳过检查会报 dirty 错误&#xA;2、如果修改了go.mod 文件，需要在make前先执行 go mod vendor &amp;amp;&amp;amp; go mod tidy</description>
    </item>
    <item>
      <title>k8s 使用</title>
      <link>http://ip:1313/docker/k8s_use-hl/index.html</link>
      <pubDate>Fri, 12 Apr 2024 09:39:39 +0000</pubDate>
      <guid>http://ip:1313/docker/k8s_use-hl/index.html</guid>
      <description>一、探针（健康状态监测） 1、探针执行方式 LivenessProbe： 判断容器是否存活 running状态，如果不健康kubelet就会杀掉pod，根据重启策略RestartPolicy进行相应的处理 ReadinessProbe：判断容器是否处于可用Ready状态，达到ready状态表示pod可以接受请求，如果不健康，从service的后端endpoint列表中把pod隔离出去 两种探针探测失败的方式不同，一个是重启容器，一个是不提供服务&#xA;2、诊断的三种方式 ExecAction: 在容器内执行指定命令。如果命令退出时返回码为0则认为诊断成功 TCPSocketAction：对指定端口上的容器的IP地址进行TCP检查，如果端口打开，则诊断认为是成功的 HTTPGetAction：对指定的端口和路径上的容器的IP地址执行HTTP Get请求，如果响应状态码大于等于200且小于400，则诊断被认为是成功的 3、Probe详细配置 initialDelaySeconds: 容器启动后第一次执行探测需要等待多少秒 periodSeconds：执行探测的频率。默认10秒，最小1秒 timeoutSeconds：探测超时时间，默认1秒，最小1秒 successThreshold：探测失败后，最少连续探测成功多少次才被认定为成功。默认1,对于liveness必须是1，最小值1 failureThreshold：探测成功后，最少连续探测失败多少次才被认定为失败，默认3，最小1 HTTP probe 中可以给httpGet设置其他配置项 4、httpget其他配置项 host：连接的主机名，默认连接到pod的IP。你可能想在http header中设置&amp;quot;Host&amp;quot;而不是使用IP。 scheme：连接使用的schema，默认HTTP。 path: 访问的HTTP server的path。 httpHeaders：自定义请求的header。HTTP运行重复的header。 port：访问的容器的端口名字或者端口号。端口号必须介于1和65535之间。 5、示例 livenessProbe: exec: command: [&amp;#34;cat&amp;#34;,&amp;#34;/app/index.html&amp;#34;] exec: command: [&amp;#34;cat&amp;#34;,&amp;#34;/app/index.html&amp;#34;]livenessProbe: exec: command: [&amp;#34;cat&amp;#34;, &amp;#34;/app/index.html&amp;#34;] # tcpSocket: # port: http initialDelaySeconds: 30 timeoutSeconds: 5 failureThreshold: 6 readinessProbe: exec: command: [&amp;#34;cat&amp;#34;, &amp;#34;/app/index.html&amp;#34;] # tcpSocket: # port: http initialDelaySeconds: 5 timeoutSeconds: 3 periodSeconds: 5二、亲和性 In: label的值在某个列表中 NotIn：label的值不在某个列表中 Exists：某个label存在 DoesNotExist：某个label不存在 Gt：label的值大于某个值（字符串比较） Lt：label的值小于某个值（字符串比较） 如果nodeAffinity中nodeSelector有多个选项，节点满足任何一个条件即可；如果matchExpressions有多个选项，则节点必须同时满足这些选项才能运行pod 。</description>
    </item>
    <item>
      <title>k8s 使用 ceph 储存</title>
      <link>http://ip:1313/docker/k8s_ceph-hl/index.html</link>
      <pubDate>Fri, 12 Apr 2024 09:39:39 +0000</pubDate>
      <guid>http://ip:1313/docker/k8s_ceph-hl/index.html</guid>
      <description>一、latest 1、创建pool ceph osd pool create k8s rbd pool init k8s2、配置 ceph-csi 1、生成 ceph client 认证 ceph auth get-or-create client.k8s mon &amp;#39;profile rbd&amp;#39; osd &amp;#39;profile rbd pool=k8s&amp;#39; mgr &amp;#39;profile rbd pool=k8s&amp;#39; [client.k8s] key = AQD9o0Fd6hQRChAAt7fMaSZXduT3NWEqylNpmg==2、生成 ceph-csi configmap ceph mon dump &amp;lt;...&amp;gt; fsid b9127830-b0cc-4e34-aa47-9d1a2e9949a8 &amp;lt;...&amp;gt; 0: [v2:192.168.1.1:3300/0,v1:192.168.1.1:6789/0] mon.a 1: [v2:192.168.1.2:3300/0,v1:192.168.1.2:6789/0] mon.b 2: [v2:192.168.1.3:3300/0,v1:192.168.1.3:6789/0] mon.ccat &amp;lt;&amp;lt;EOF &amp;gt; csi-config-map.yaml --- apiVersion: v1 kind: ConfigMap data: config.json: |- [ { &amp;#34;clusterID&amp;#34;: &amp;#34;b9127830-b0cc-4e34-aa47-9d1a2e9949a8&amp;#34;, &amp;#34;monitors&amp;#34;: [ &amp;#34;192.</description>
    </item>
    <item>
      <title>k8s 安装部署</title>
      <link>http://ip:1313/docker/k8s_deploy-hl/index.html</link>
      <pubDate>Fri, 12 Apr 2024 09:39:39 +0000</pubDate>
      <guid>http://ip:1313/docker/k8s_deploy-hl/index.html</guid>
      <description>一、v2.13.2 1、v2.13.2 git checkout v2.13.22、main.yml vim /root/kubespray/roles/download/defaults/main.yml kube_version: v1.17.7 etcd_version: v3.3.12 gcr_image_repo: &amp;#34;local.com/k8s/gcr.io&amp;#34; kube_image_repo: &amp;#34;local.com/k8s/k8s.gcr.io&amp;#34; docker_image_repo: &amp;#34;local.com/k8s&amp;#34; quay_image_repo: &amp;#34;local.com/k8s/quay.io&amp;#34; calico_version: &amp;#34;v3.13.2&amp;#34; calico_ctl_version: &amp;#34;v3.13.2&amp;#34; calico_cni_version: &amp;#34;v3.13.2&amp;#34; calico_policy_version: &amp;#34;v3.13.2&amp;#34; calico_typha_version: &amp;#34;v3.13.2&amp;#34; typha_enabled: false flannel_version: &amp;#34;v0.12.0&amp;#34; cni_version: &amp;#34;v0.8.6&amp;#34; kubelet_download_url: &amp;#34;http://192.168.0.242:88/kubelet&amp;#34; kubectl_download_url: &amp;#34;http://192.168.0.242:88/kubectl&amp;#34; kubeadm_download_url: &amp;#34;http://192.168.0.242:88/kubeadm&amp;#34; etcd_download_url: &amp;#34;http://192.168.0.242:88/etcd-v3.3.12-linux-amd64.tar.gz&amp;#34; cni_download_url: &amp;#34;http://192.168.0.242:88/cni-plugins-linux-amd64-v0.8.6.tgz&amp;#34; calicoctl_download_url: &amp;#34;http://192.168.0.242:88/calicoctl-linux-amd64&amp;#34; crictl_download_url: &amp;#34;http://192.168.0.242:88/crictl-v1.18.0-linux-amd64.tar.gz&amp;#34;3、k8s-cluster.yml vim /root/kubespray/inventory/mycluster/group_vars/k8s-cluster/k8s-cluster.yml kube_version: v1.18.4 kube_image_repo: &amp;#34;local.com/k8s/k8s.gcr.io&amp;#34;4、hosts vim /etc/hosts 192.168.0.242 local.com5、certs.d mkdir /etc/docker/certs.d/local.com二、使用kubespray安装(kubespray: v2.11.0, k8s:v1.15.3) 1、查看系统是否支持虚拟化 egrep --color &amp;#39;vmx|svm&amp;#39; /proc/cpuinfo2、关闭和禁用防火墙 systemctl stop firewalld systemctl disable firewalld setenforce 0 sed -i &amp;#34;s/^SELINUX=enforcing/SELINUX=disabled/g&amp;#34; /etc/selinux/config3、禁用交换分区 swapoff -a sed -i &amp;#39;s/.</description>
    </item>
    <item>
      <title>k8s 相关</title>
      <link>http://ip:1313/docker/k8s-hl/index.html</link>
      <pubDate>Fri, 12 Apr 2024 09:39:39 +0000</pubDate>
      <guid>http://ip:1313/docker/k8s-hl/index.html</guid>
      <description>一、相关概念 k8s components&#xA;​&#x9;Kubernetes 是一个开源容器编排引擎，用于容器化应用的自动化部署、扩展、管理、自动扩缩容、维护等功能。k8s 最核心的两个设计理念容错性和易扩展性，容错性保证 k8s 系统稳定性和安全性的基础，易扩展性是保证 k8s 对变更友好，可以快速迭代增加新功能的基础。&#xA;功能：&#xA;快速部署应用（自动化容器部署和复制） 快速扩展应用（随时扩展或收缩容器规模） 无缝对接新应用功能（很容易升级应用到新版本） 提供容器弹性，如果容器失效就替换它 将容器组织成组，并提供容器间的负载均衡 节省资源，优化硬件资源的使用 特点：&#xA;可移植：支持公有云、私有云、混合云、多重云（muti-cloud） 可扩展：模块化、插件化、可挂载、可组合 自动化：自动部署、自动重启、自动复制、自动伸缩/扩展 优势：&#xA;容器编排 轻量级 开源 弹性伸缩 负载均衡 控制平面组件： 名称 说明 API Server API服务是控制平面的访问入口 Etcd 保存集群数据的键值对存储服务 Scheduler 调度Pod在工作节点上运行 kube-controllermanager 管理云平台无关的控制器 cloud-controllermanager 管理所有与云平台相关的控制器，这是为了把云平台相关的代码与k8s自身进行分离 节点平面组件： 名称 说明 kubelet 确保Pod中的容器运行正常 kube-proxy 用来实现服务的网络代理 container runtime 负责运行容器的软件，如Docker 核心思想 资源注册 发现框架 Pod k8s 基本调度单元，表示由一个或多个容器组成的分组。Pod 里面的容器共享一些资源，包括存储、网络和每个容器的运行配置。每个 Pod 都有唯一的 IP 地址，其中的容器共享同一个 IP 地址和端口范围，相互之间可以通过 localhost 访问。Pod 还可以使它的容器访问其定义的共享的存储卷（volume），通过这种方式，可以实现 Pod 容器之间的数据共享。</description>
    </item>
    <item>
      <title>kubebuilder</title>
      <link>http://ip:1313/docker/kubebuilder-hl/index.html</link>
      <pubDate>Fri, 12 Apr 2024 09:39:39 +0000</pubDate>
      <guid>http://ip:1313/docker/kubebuilder-hl/index.html</guid>
      <description>一、环境准备 1、容器环境 docker run -itd --name go_dev -w /go/src -v /go_src:/go/src/ golang:1.17.6 bash docker exec -it go_dev bash2、安装kubebuilder curl -L -o kubebuilder https://go.kubebuilder.io/dl/latest/$(go env GOOS)/$(go env GOARCH) chmod +x kubebuilder &amp;amp;&amp;amp; mv kubebuilder /usr/local/bin/3、设置 goproxy go env -w GO111MODULE=on &amp;amp;&amp;amp; go env -w GOPROXY=https://goproxy.cn,direct4、编译环境（可选） FROM golang:1.17.6 ARG http_proxy=http://192.168.0.1:1080 ENV http_proxy=$http_proxy ENV https_proxy=$http_proxy WORKDIR /go/src RUN curl -L -o kubebuilder https://go.kubebuilder.io/dl/latest/$(go env GOOS)/$(go env GOARCH) &amp;amp;&amp;amp; \ chmod +x kubebuilder &amp;amp;&amp;amp; mv kubebuilder /usr/local/bin/ &amp;amp;&amp;amp; \ go env -w GO111MODULE=on &amp;amp;&amp;amp; go env -w GOPROXY=https://goproxy.</description>
    </item>
    <item>
      <title>kubeedge</title>
      <link>http://ip:1313/docker/kubeedge-hl/index.html</link>
      <pubDate>Fri, 12 Apr 2024 09:39:39 +0000</pubDate>
      <guid>http://ip:1313/docker/kubeedge-hl/index.html</guid>
      <description>一、要求 云端环境： OS: Ubuntu Server 20.04.1 LTS 64bit Kubernetes: v1.19.8 网络插件：calico v3.16.3 Cloudcore: kubeedge/cloudcore:v1.6.1 边缘环境： OS: Ubuntu Server 18.04.5 LTS 64bit EdgeCore: v1.19.3-kubeedge-v1.6.1 docker: version: 20.10.7 cgroupDriver: systemd二、安装k8s 略三、安装golang wget -O /usr/local/go1.16.4.linux-amd64.tar.gz https://golang.org/dl/go1.16.4.linux-amd64.tar.gz tar -C /usr/local -zxvf /usr/local/go1.16.4.linux-amd64.tar.gz echo &amp;#34;export PATH=$PATH:/usr/local/go/bin&amp;#34; |tee &amp;gt;&amp;gt; /etc/profile source /etc/profilegit clone -b v1.8.2 --deph 1 https://github.com/kubeedge/kubeedge.gitcurl -LO https://github.com/kubeedge/kubeedge/releases/download/v1.8.2/kubeedge-v1.8.2-linux-amd64.tar.gzcurl -LO https://github.com/kubeedge/kubeedge/releases/download/v1.8.2/keadm-v1.8.2-linux-amd64.tar.gzcurl -LO https://github.com/kubeedge/kubeedge/releases/download/v1.8.2/edgesite-v1.8.2-linux-amd64.tar.gzkeadm init --kube-config=$KUBECONFIG --advertise-address=10.0.0.19 keadm init --kube-config=./config --advertise-address=10.0.0.19export https_proxy=http://192.168.0.184:1080 export http_proxy=http://192.168.0.184:1080 export HTTPS_PROXY=http://192.168.0.184:1080 export HTTP_PROXY=http://192.</description>
    </item>
    <item>
      <title>openshift 相关</title>
      <link>http://ip:1313/docker/openshift-hl/index.html</link>
      <pubDate>Fri, 12 Apr 2024 09:39:39 +0000</pubDate>
      <guid>http://ip:1313/docker/openshift-hl/index.html</guid>
      <description>Master Node提供的组件：API Server (负责处理客户端请求, 包括node、user、administrator和其他的infrastructure系统)；Controller Manager Server (包括scheduler和replication controller)；OpenShift客户端工具 (oc)&#xA;Compute Node(Application Node) 部署application&#xA;Infra Node 运行router、image registry和其他的infrastructure服务(由管理员安装的系统服务Application)&#xA;etcd 可以部署在Master Node，也可以单独部署， 用来存储共享数据：master state、image、 build、deployment metadata等&#xA;Pod 最小的Kubernetes object，可以部署一个或多个container&#xA;https://blog.51cto.com/7308310/2171091&#xA;一、单节点部署 v3.11 1、安装相关软件 yum -y install epel-release git python-pip gcc python-devel centos-release-openshift-origin3112、下载源码 https://github.com/openshift/openshift-ansible/tree/v3.11&#xA;git clone https://github.com/openshift/openshift-ansible.git cd openshift-ansible git tag git checkout v3.11 git describe3、修改依赖ansible的版本为2.7.x vim requirements.txt ansible==2.7.124、安装依赖 pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple5、编写hosts.localhost cat inventory/hosts.localhost #bare minimum hostfile [OSEv3:children] masters nodes etcd [OSEv3:vars] # if your target hosts are Fedora uncomment this #ansible_python_interpreter=/usr/bin/python3 openshift_deployment_type=origin openshift_portal_net=172.</description>
    </item>
    <item>
      <title>overlay2</title>
      <link>http://ip:1313/docker/overlay2-hl/index.html</link>
      <pubDate>Fri, 12 Apr 2024 09:39:39 +0000</pubDate>
      <guid>http://ip:1313/docker/overlay2-hl/index.html</guid>
      <description>一、查看镜像 id docker images |grep 3.9-alpine3.12 python 3.9-alpine3.12 7254f7459375 3 months ago 44.2MB二、查看镜像层 cat /var/lib/docker/image/overlay2/imagedb/content/sha256/7254f7459375ca357ad951c2a97d548455dc9bcc0064589c519ff7dc37708d7d|python3 -m json.tool ... &amp;#34;rootfs&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;layers&amp;#34;, &amp;#34;diff_ids&amp;#34;: [ &amp;#34;sha256:f4666769fca7a1db532e3de298ca87f7e3124f74d17e1937d1127cb17058fead&amp;#34;, &amp;#34;sha256:f2bc6754fc86dfbd965dfb93127dc473c36bc44cf21f87a779dcd0f94d924f3d&amp;#34;, &amp;#34;sha256:e25ee6f7192be198a37c00741fce057aaa0eef58b0fb77a9fc2a23dcd84a5302&amp;#34;, &amp;#34;sha256:0d2c65b855c43c1c83cf24e866e70e232f3ba451aa6aa159adc1e4dc3d51f96a&amp;#34;, &amp;#34;sha256:debde983e4fe3ad328c6aa3f37cf9ee30d268b8266e37da87417133d0cc252ad&amp;#34; ] } ...docker inspect python:3.9-alpine3.12 ... &amp;#34;RootFS&amp;#34;: { &amp;#34;Type&amp;#34;: &amp;#34;layers&amp;#34;, &amp;#34;Layers&amp;#34;: [ &amp;#34;sha256:f4666769fca7a1db532e3de298ca87f7e3124f74d17e1937d1127cb17058fead&amp;#34;, &amp;#34;sha256:f2bc6754fc86dfbd965dfb93127dc473c36bc44cf21f87a779dcd0f94d924f3d&amp;#34;, &amp;#34;sha256:e25ee6f7192be198a37c00741fce057aaa0eef58b0fb77a9fc2a23dcd84a5302&amp;#34;, &amp;#34;sha256:0d2c65b855c43c1c83cf24e866e70e232f3ba451aa6aa159adc1e4dc3d51f96a&amp;#34;, &amp;#34;sha256:debde983e4fe3ad328c6aa3f37cf9ee30d268b8266e37da87417133d0cc252ad&amp;#34; ] }, ...三、查看元数据信息及 rootfs 1、第一层 ls /var/lib/docker/image/overlay2/layerdb/sha256/f4666769fca7a1db532e3de298ca87f7e3124f74d17e1937d1127cb17058fead/ cache-id diff size tar-split.json.gz cat cache-id a9410091d624483c6ea184a139807000b39e9ee2d4e6fda29ecbfb7d7a914856 ls /var/lib/docker/overlay2/a9410091d624483c6ea184a139807000b39e9ee2d4e6fda29ecbfb7d7a914856 committed diff link ls /var/lib/docker/overlay2/a9410091d624483c6ea184a139807000b39e9ee2d4e6fda29ecbfb7d7a914856/diff/ bin dev etc home lib media mnt opt proc root run sbin srv sys tmp usr var cat link ROYFFPZS3DDRNW5YDCZ4RC6TKS ls /var/lib/docker/overlay2/l/ROYFFPZS3DDRNW5YDCZ4RC6TKS/ bin dev etc home lib media mnt opt proc root run sbin srv sys tmp usr var2、第二层 echo -n &amp;#34;sha256:f4666769fca7a1db532e3de298ca87f7e3124f74d17e1937d1127cb17058fead sha256:f2bc6754fc86dfbd965dfb93127dc473c36bc44cf21f87a779dcd0f94d924f3d&amp;#34;|sha256sum 25370907defafe1eacf55dd1df121dd16890773e4a3d0637629de56ed6c69198 ls /var/lib/docker/image/overlay2/layerdb/sha256/25370907defafe1eacf55dd1df121dd16890773e4a3d0637629de56ed6c69198/ cache-id diff parent size tar-split.</description>
    </item>
    <item>
      <title>rancher 相关</title>
      <link>http://ip:1313/docker/rancher-hl/index.html</link>
      <pubDate>Fri, 12 Apr 2024 09:39:39 +0000</pubDate>
      <guid>http://ip:1313/docker/rancher-hl/index.html</guid>
      <description>一、编译rancher dashborad 1、依赖安装 wget https://nodejs.org/dist/v16.14.2/node-v16.14.2-linux-x64.tar.xz tar -vxf node-v16.14.2-linux-x64.tar.xz cp -r node-v16.14.2-linux-x64 /usr/local/ cd node-v16.14.2-linux-x64/ ./bin/node -v v16.14.2 cd /usr/local/node-v16.14.2-linux-x64/ ln -s /usr/local/node-v16.14.2-linux-x64/bin/npm /usr/bin/ ln -s /usr/local/node-v16.14.2-linux-x64/bin/node /usr/bin/ npm config set registry=https://registry.npm.taobao.org npm install -g cnpm --registry=https://registry.npm.taobao.org npm install --global yarn yarn config set registry https://registry.npm.taobao.org 可能存在preset-env报错问题&#xA;npm uninstall @babel/preset-env npm install @babel/preset-env@7.12.132、下载运行 依赖rancher server的运行，需提前部署&#xA;git clone https://github.com/rancher/dashboard.git cd dashboard/ yarn install API=https://192.168.0.31:30000 yarn dev3、容器开发 docker run -it --net=host --name dev -v /git/dashboard:/src rancher/dashboard:dev bash4、代码修改 vim pkg/settings/setting.</description>
    </item>
    <item>
      <title>容器交叉编译</title>
      <link>http://ip:1313/docker/multiarch-hl/index.html</link>
      <pubDate>Fri, 12 Apr 2024 09:39:39 +0000</pubDate>
      <guid>http://ip:1313/docker/multiarch-hl/index.html</guid>
      <description>一、gcc-linaro gcc-linaro-7.5.0&#xA;1、dockerfile From ubuntu:20.04 ENV PATH=$PATH:/root/gcc-linaro-7.5.0-2019.12-x86_64_aarch64-linux-gnu/bin ENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/root/gcc-linaro-7.5.0-2019.12-x86_64_aarch64-linux-gnu/lib WORKDIR /root RUN sed -i s@/archive.ubuntu.com/@/mirrors.ustc.edu.cn/@g /etc/apt/sources.list &amp;amp;&amp;amp; \ apt update &amp;amp;&amp;amp; \ DEBIAN_FRONTEND=noninteractive apt install -y dracut git make libncurses-dev libelf-dev bison flex libssl-dev bc &amp;amp;&amp;amp; \ git clone http://192.168.0.90/kdpf/gcc-linaro-7.5.0-2019.12-x86_64_aarch64-linux-gnu.git &amp;amp;&amp;amp; \ alias make=&amp;#39;make ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu-&amp;#39;2、make_kernel #!/bin/bash rm -rf out_put/* docker run -it --rm -v $PWD/out:/out gcc-linaro:1.0.0 /bin/sh -c &amp;#34;sh &amp;lt;&amp;lt; EOF alias make=&amp;#39;make ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu- -j20&amp;#39; git clone -b build --depth=1 http://192.</description>
    </item>
    <item>
      <title>容器基础镜像构建</title>
      <link>http://ip:1313/docker/debootstrap-hl/index.html</link>
      <pubDate>Fri, 12 Apr 2024 09:39:39 +0000</pubDate>
      <guid>http://ip:1313/docker/debootstrap-hl/index.html</guid>
      <description>构建基础的 rootfs —&amp;gt; 配置基础系统参数 —&amp;gt; 部署用户自定义软件 —&amp;gt; 清理系统 —&amp;gt; 打包为容器镜像 —&amp;gt; 测试镜像 —&amp;gt; 发布仓库&#xA;https://docs.docker.com/develop/develop-images/baseimages/&#xA;一、Ubuntu 1、安装 debootstrap apt install -y debootstrap2、创建 rootfs 存放位置 mkdir -p /opt/diros cd /opt/diros3、构建基础 Ubuntu 20.10 groovy 的 rootfs debootstrap --verbose --arch=amd64 groovy /opt/diros http://mirrors.aliyun.com/ubuntu #debootstrap --verbose --arch=amd64 bionic /opt/diros https://mirrors.tuna.tsinghua.edu.cn/ubuntu4、配置基础系统参数 chroot /opt/diros/ /bin/bashapt update apt upgrade apt -y install vim localesdpkg-reconfigure locales cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime tee /etc/lsb-release &amp;lt;&amp;lt;-&amp;#39;EOF&amp;#39; ID=Intewell NAME=&amp;#34;Intewell-Linux&amp;#34; VERSION=&amp;#34;&amp;#34; VERSION_ID= EOFrm -rf /tmp/* apt clean exit5、打包并创建 docker 镜像 tar -C /opt/diros/ -cv .</description>
    </item>
    <item>
      <title>将ISO镜像转换为docker镜像</title>
      <link>http://ip:1313/docker/convert_iso_to_docker_img-hl/index.html</link>
      <pubDate>Fri, 12 Apr 2024 09:39:39 +0000</pubDate>
      <guid>http://ip:1313/docker/convert_iso_to_docker_img-hl/index.html</guid>
      <description>一、转换 1、准备 iso 例： ubuntu-16.04.6-desktop-amd64.iso&#xA;2、安装工具 squashfs-tools yum install -y squashfs-tools3、创建两个目录 mkdir rootfs unquashfs4、挂载 iso mount -o loop ubuntu-16.04.6-desktop-amd64.iso rootfs5、找到 filesystem.squashfs.file 文件路径 find . -type f | grep filesystem.squashfsfind . -type f | grep filesystem.squashfs ./rootfs/casper/filesystem.squashfs ./rootfs/casper/filesystem.squashfs.gpg6、使用 unsquashfs 解压 filesystem.squashfs 到 unsquashfs 文件夹 unsquashfs -f -d unsquashfs/ rootfs/casper/filesystem.squashfs7、压缩并导入到 docker tar -C unsquashfs -c . | docker import - ubuntu/myimg8、查看 docker images|grep &amp;#34;ubuntu/mying&amp;#34;二、安装桌面 1、clone 以下项目 https://github.com/hlyani/docker-ubuntu-desktop.git&#xA;git clone https://github.com/hlyani/docker-ubuntu-desktop.git2、修改 dockerfile 文件，将镜像改为刚才构建好的 FROM ubuntu/myimg ENV DEBIAN_FRONTEND noninteractive ENV USER root COPY sources.</description>
    </item>
    <item>
      <title>常用软件安装</title>
      <link>http://ip:1313/docker/usual_software-hl/index.html</link>
      <pubDate>Fri, 12 Apr 2024 09:39:39 +0000</pubDate>
      <guid>http://ip:1313/docker/usual_software-hl/index.html</guid>
      <description>1、rocket https://hub.docker.com/_/rocket-chat https://rocket.chat/install&#xA;docker run -d --name rocketchat-mongo mongo:4.0.10 --smallfiles --oplogSize 128 --replSet rs1 --storageEngine=mmapv1docker exec -d rocketchat-mongo bash -c &amp;#39;echo -e &amp;#34;replication:\n replSetName: \&amp;#34;rs01\&amp;#34;&amp;#34; | tee -a /etc/mongod.conf &amp;amp;&amp;amp; mongo --eval &amp;#34;printjson(rs.initiate())&amp;#34;&amp;#39;docker run -d --name rocketchat --link rocketchat-mongo -e &amp;#34;MONGO_URL=mongodb://rocketchat-mongo:27017/rocketchat&amp;#34; -e MONGO_OPLOG_URL=mongodb://rocketchat-mongo:27017/local?replSet=rs01 -e ROOT_URL=http://192.168.21.87:3001 -p 3001:3000 rocketchat/rocket.chat:1.2.12、samba https://github.com/dperson/samba&#xA;-s &amp;#34;&amp;lt;name;/path&amp;gt;[;browse;readonly;guest;users;admins;writelist;comment]&amp;#34; Configure a share required arg: &amp;#34;&amp;lt;name&amp;gt;;&amp;lt;/path&amp;gt;&amp;#34; &amp;lt;name&amp;gt; is how it&amp;#39;s called for clients &amp;lt;path&amp;gt; path to share NOTE: for the default values, just leave blank [browsable] default:&amp;#39;yes&amp;#39; or &amp;#39;no&amp;#39; [readonly] default:&amp;#39;yes&amp;#39; or &amp;#39;no&amp;#39; [guest] allowed default:&amp;#39;yes&amp;#39; or &amp;#39;no&amp;#39; NOTE: for user lists below, usernames are separated by &amp;#39;,&amp;#39; [users] allowed default:&amp;#39;all&amp;#39; or list of allowed users [admins] allowed default:&amp;#39;none&amp;#39; or list of admin users [writelist] list of users that can write to a RO share [comment] description of share docker run -it --name samba -p 139:139 -p 445:445 \ --restart always \ -e TZ=EST5EDT \ -v /fs/samba:/mount \ -d dperson/samba -p \ -s &amp;#34;samba;/mount/;yes;no;yes;all;all;all;all&amp;#34;mkdir /opt/test chmod 777 -R /opt/testdocker run -it -p 139:139 -p 445:445 --name samba -v /opt/test:/mount -d dperson/samba \ -u &amp;#34;test;qwe&amp;#34; \ -s &amp;#34;test;/mount/;yes;no;yes;all;all;all&amp;#34; \ -w &amp;#34;WORKGROUP&amp;#34; \ -g &amp;#34;force user= test&amp;#34; \ -g &amp;#34;guest account= test&amp;#34;# 查看正在运行的配置参数 testparm -v3、gitlab https://docs.</description>
    </item>
    <item>
      <title>挂载USB到docker容器</title>
      <link>http://ip:1313/docker/mount_usb_to_docker-hl/index.html</link>
      <pubDate>Fri, 12 Apr 2024 09:39:39 +0000</pubDate>
      <guid>http://ip:1313/docker/mount_usb_to_docker-hl/index.html</guid>
      <description>一、基础准备 1、部署 docker 环境，拉取或构建带vnc的docker镜像 https://hub.docker.com/r/x11vnc/desktop&#xA;git clone https://github.com/hlyani/docker-ubuntu-vnc-desktop.gitgit clone https://github.com/hlyani/x11vnc-desktop.gitdocker pull x11vnc/desktopdocker pull centminmod/docker-ubuntu-vnc-desktop3、查看主机 usb 设备 apt -y install usbutilslsusb -D /dev/bus/usb/001/001 lsusb -s 001:0014、查看视频设备 ls /dev/video*5、运行 docker 在运行docker时添加&amp;ndash;privileged，这样可以放开 docker 的所有系统操作权限，但是这种操作带来的安全风险比较大。&#xA;docker run -itu0 --rm -p 6080:6080 --privileged=true --name test --device=/dev/video0 --device=/dev/video1 -v /dev/bus/usb:/dev/bus/usb x11vnc/desktopdocker run -itu0 --name test --privileged=true --net=host -v /dev/bus/usb:/dev/bus/usb ubuntu bashdocker run -itu0 --name test --privileged=true --net=host -v /dev/bus/usb:/dev/bus/usb ubuntu bashdocker run -it --rm --device=/dev/video0 -e DISPLAY=unix$DISPLAY -v /tmp/.</description>
    </item>
  </channel>
</rss>